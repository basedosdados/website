---
title: Dados
category: Contribua
order: 0
---

# Dados

## Por que minha organiza√ß√£o deve subir dados na BD?

- **Capacidade de cruzar suas bases com dados de diferentes
  organiza√ß√µes** de forma simples e f√°cil. J√° s√£o centenas de conjuntos
  de dados p√∫blicos das maiores organiza√ß√µes do Brasil e do mundo presentes
  no nosso *datalake*.

- **Compromisso com a transpar√™ncia, qualidade dos dados e
  desenvolvimento de melhores pesquisas, an√°lises e solu√ß√µes** para a
  sociedade. N√£o s√≥ democratizamos o acesso a dados abertos, mas tamb√©m dados
  de qualidade. Temos um time especializado que revisa e garante a qualidade dos
  dados adicionados ao *datalake*.

- **Participa√ß√£o de uma comunidade que cresce cada vez mais**: milhares
  de jornalistas, pesquisadores(as), desenvolvedores(as), j√° utilizam e
  acompanham a Base dos Dados.
  {/* TODO: Colocar aqui o link do nosso painel de audiencia quando tiver pronto :) */}

## Passo a passo para subir dados

Quer subir dados na BD e nos ajudar a construir esse reposit√≥rio?
*Maravilha!* Organizamos tudo o que voc√™ precisa no manual abaixo em 8 passos

Para facilitar a explica√ß√£o, vamos seguir um exemplo j√° pronto com dados da [RAIS](/dataset/3e7c4d58-96ba-448e-b053-d385a829ef00?table=86b69f96-0bfe-45da-833b-6edc9a0af213).

<Tip caption="Voc√™ pode navegar pelas etapas no menu √† esquerda">

 Sugerimos fortemente que entre em nosso [canal no Discord](https://discord.gg/huKWpsVYx4) para tirar d√∫vidas e interagir com a equipe e outros(as) colaboradores(as)! üòâ
</Tip>

### Antes de come√ßar

Alguns conhecimentos s√£o necess√°rias para realizar esse processo:

- **Python, R e/ou SQL**: para criar os c√≥digos de captura e limpeza dos dados.
- **Linha de comando**: para configurar seu ambiente local
  e conex√£o com o Google Cloud.
- **Github**: para subir seu c√≥digo para revis√£o da
  nossa equipe.

<Tip caption="N√£o tem alguma dessas habilidades, mas quer colaborar?">

Temos um time de dados que pode te ajudar, basta entrar no [nosso Discord](https://discord.gg/huKWpsVYx4) e mandar uma mensagem em #quero-contribuir.
</Tip>

### Como funciona o processo?

- [1. Escolher a base e entender mais dos dados](#1-escolher-a-base-e-entender-mais-dos-dados) - primeiro precisamos conhecer o que estamos tratando.
- [2. Baixar nossa pasta template](#2-baixar-nossa-pasta-template) - √© hora estruturar o trabalho a ser feito
- [3. Preencher as tabelas de arquitetura](#3-preencher-as-tabelas-de-arquitetura) - √© primordial definir a estrutura dos dados antes de iniciarmos o tratamento
- [4. Escrever codigo de captura e limpeza de dados](#4-escrever-codigo-de-captura-e-limpeza-de-dados) - hora de botar a m√£o na massa!
- [5. (Caso necess√°rio) Organizar arquivos auxiliares](#5-caso-necessario-organizar-arquivos-auxiliares) -  porque at√© dados precisam de guias
- [6. (Caso necess√°rio) Criar tabela dicion√°rio](#6-caso-necessario-criar-tabela-dicionario) - momento de montar os dicion√°rios
- [7. Subir tudo no Google Cloud](#7-subir-tudo-no-google-cloud) - afinal, √© por l√° que ficam os dados da BD
- [8. Enviar tudo para revis√£o](#8-enviar-tudo-para-revisao) - um olhar da nossa equipe para garantir que tudo est√° pronto para ir pra produ√ß√£o!


### 1. Escolher a base e entender mais dos dados


Mantemos a lista de conjuntos para volunt√°rios no nosso [Github](https://github.com/orgs/basedosdados/projects/17/views/9). Para come√ßar a subir uma base do seu interesse, basta abrir uma [nova issue](https://github.com/basedosdados/pipelines/issues/new?template=new-data.yml) de dados. Caso sua base (conjunto) j√° esteja listada, basta marcar seu usu√°rio do Github como `assignee`

Seu primeiro trabalho √© preencher as informa√ß√µes na issue. Essas informa√ß√µes v√£o te ajudar a entender melhor os dados e ser√£o muito √∫teis para o tratamento e o preenchimento de metadados.

Quando finalizar essa etapa, chame algu√©m da equipe dados para que as informa√ß√µes que voc√™ mapeou sobre o conjunto j√° entrem pro nosso site!

### 2. Baixar nossa pasta template

[Baixe aqui a pasta
_template_](https://drive.google.com/drive/folders/1xXXon0vdjSKr8RCNcymRdOKgq64iqfS5?usp=sharing)
 e renomeie para o `<dataset_id>` (definido na issue do [passo 1](#-1-Escolher-a-base-e-entender-mais-dos-dados)). Essa pasta template facilita e organiza todos os
passos daqui pra frente. Sua
estrutura √© a seguinte:

- `<dataset_id>/`
    - `code/`: C√≥digos necess√°rios para **captura** e **limpeza** dos dados
    ([vamos ver mais no passo
    4](#4-escrever-codigo-de-captura-e-limpeza-de-dados)).
    - `input/`: Cont√©m todos os arquivos com dados originais, exatamente
    como baixados da fonte prim√°ria. ([vamos ver mais no passo
    4](#4-escrever-codigo-de-captura-e-limpeza-de-dados)).
    - `output/`: Arquivos finais, j√° no formato pronto para subir na BD ([vamos ver mais no passo
    4](#4-escrever-codigo-de-captura-e-limpeza-de-dados)).
    - `tmp/`: Quaisquer arquivos tempor√°rios criados pelo c√≥digo em `/code` no processo de limpeza e tratamento ([vamos ver mais no passo
    4](#4-escrever-codigo-de-captura-e-limpeza-de-dados)).
    - `extra/`
        - `architecture/`: Tabelas de arquitetura ([vamos ver mais no passo 3](#3-preencher-as-tabelas-de-arquitetura)).
        - `auxiliary_files/`: Arquivos auxiliares aos dados ([vamos ver mais no passo 5](#5-caso-necess√°rio-organizar-arquivos-auxiliares)).
        - `dicionario.csv`: Tabela dicion√°rio de todo o conjunto de dados ([vamos ver mais no passo 6](#6-caso-necess√°rio-criar-tabela-dicion√°rio)).

> Apenas a pasta `code` ser√° commitada para o seu projeto, os demais arquivos existir√£o apenas localmente ou no Google Cloud.

### 3. Preencher as tabelas de arquitetura

As tabelas de arquitetura determinam **qual a estrutura de
cada tabela do seu conjunto de dados**. Elas definem, por exemplo, o nome, ordem e metadados das vari√°veis, al√©m de compatibiliza√ß√µes quando h√° mudan√ßas em vers√µes (por
exemplo, se uma vari√°vel muda de nome de um ano para o outro).

> Cada tabela do conjunto de dados deve ter sua pr√≥pria tabela de arquitetura (planilha), que deve ser preenchida no **Google Drive** para permitir a corre√ß√£o pela nossa equipe de dados.


#### Exemplo: RAIS - Tabelas de arquitetura

As tabelas de arquitetura da RAIS [podem ser consultadas aqui](https://docs.google.com/spreadsheets/d/1dPLUCeE4MSjs0ykYUDsFd-e7-9Nk6LVV/edit?usp=sharing&ouid=103008455637924805982&rtpof=true&sd=true). Elas s√£o uma √≥tima refer√™ncia para voc√™ come√ßar seu trabalho j√° que tem muitas vari√°veis e exemplos de diversas situa√ß√µes que voc√™ pode acabar encontrando.

#### Para o preenchimento de cada tabela do seu conjunto siga esse passo a passo:

<Tip caption="A cada in√≠cio e final de etapa consulte nosso [manual de estilo](style_data) para garantir que voc√™ est√° seguindo a padroniza√ß√£o da BD"/>


1. Listar todas as vari√°veis dos dados na coluna `original_name`
    - Obs: Caso a base mude o nome das vari√°veis ao longo dos anos (como a RAIS), √© necess√°rio fazer a compatibiliza√ß√£o entre anos para todas as vari√°veis preenchendo a coluna de `original_name_YYYY` para cada ano ou m√™s dispon√≠vel
2. Renomear as vari√°veis conforme nosso [manual](style_data) na coluna `name`
3. Entender o tipo da vari√°vel e preencher a coluna `bigquery_type`
4. Preencher a descri√ß√£o em `description` conforme o [manual](style_data)
5. A partir da compatibiliza√ß√£o entre anos e/ou consultas aos dados brutos, preencher a cobertura temporal em `temporal_coverage` de cada vari√°vel
    - Obs: Caso as vari√°veis tenham a mesma cobertura temporal da tabela preencher apenas com '(1)'
6. Indicar com 'yes' ou 'no' se h√° dicion√°rio para as vari√°veis em `covered_by_dictionary`
7. Verificar se as vari√°veis representam alguma entidade presente nos [diret√≥rios](/dataset/33b49786-fb5f-496f-bb7c-9811c985af8e?table=0a2d8187-f936-437d-89db-b4eb3a7e1735) para preencher o `directory_column`
8. Para as vari√°veis do tipo `int64` ou `float64` verificar se √© necess√°rio incluir uma [unidade de medida](https://github.com/basedosdados/website/blob/master/ckanext-basedosdados/ckanext/basedosdados/validator/available_options/measurement_unit.py)
9. Reordernar as vari√°veis conforme o [manual](style_data)

<Tip caption="Quando terminar de preencher as tabelas de arquitetura, entre em contato com a equipe da Base dos Dados para validar tudo.">

√â necess√°rio que esteja claro o formato final que os dados devem ficar _antes_ de come√ßar a escrever o c√≥digo. Assim a gente evita o retrabalho.

</Tip>

### 4. Escrever codigo de captura e limpeza de dados

Ap√≥s validadas as tabelas de arquitetura, podemos escrever os c√≥digos de
**captura** e **limpeza** dos dados.

- **Captura**: C√≥digo que baixa automaticamente todos os dados originais e os salva em `/input`. Esses dados podem estar dispon√≠veis em portais ou links FTP, podem ser raspados de sites, entre outros.

- **Limpeza**: C√≥digo que transforma os dados originais salvos em `/input` em dados limpos, salva na pasta `/output`, para, posteriormente, serem subidos na BD.

Cada tabela limpa para produ√ß√£o pode ser salva como um arquivo √∫nico ou, caso seja muito grande (e.g. acima de 200 mb), ser particionada no formato [Hive](https://cloud.google.com/bigquery/docs/hive-partitioned-loads-gcs) em v√°rios sub-arquivos. Os formatos aceitos s√£o `.csv` ou `.parquet`. Nossa recomenda√ß√£o √© particionar tabelas por `ano`, `mes` e `sigla_uf`. O particionamento √© feito atrav√©s da estrutura de pastas, veja o exemplo a baixo para visualizar como.

#### Exemplo: RAIS - Particionamento

A tabela `microdados_vinculos` da RAIS Vinculos, por exemplo, √© uma tabela muito grande (+400GB) por isso n√≥s particionamos por `ano` e `sigla_uf`. O particionamento foi feito usando a estrutura de pastas `/microdados_vinculos/ano=YYYY/sigla_uf=XX` .

#### Padr√µes necess√°rios no c√≥digo

- Devem ser escritos em [Python](https://www.python.org/) ou [R](https://www.r-project.org/) -
  para que a revis√£o possa ser realizada pela equipe.
- Pode estar em script (`.py`, `.R`, ...) ou *notebooks* (Google Colab, Jupyter, Rmarkdown, etc).
- Os caminhos de arquivos devem ser atalhos _relativos_ √† pasta ra√≠z
  (`<dataset_id>`), ou seja, n√£o devem depender dos caminhos do seu
  computador.
- A limpeza deve seguir nosso [manual de estilo](style_data) e as [melhores pr√°ticas de programa√ß√£o](https://en.wikipedia.org/wiki/Best_coding_practices).

#### Exemplo: PNAD Cont√≠nua - C√≥digo de limpeza

O c√≥digo de limpeza foi constru√≠do em R e [pode ser consultado
aqui](https://github.com/basedosdados/sdk/tree/master/bases/br_ibge_pnadc/code).

#### Exemplo: Atividade na C√¢mara Legislativa - C√≥digo de download e limpeza
O c√≥digo de limpeza foi constru√≠do em Python [pode ser consultado aqui](https://github.com/basedosdados/sdk/tree/bea9a323afcea8aa1609e9ade2502ca91f88054c/bases/br_camara_atividade_legislativa/code)



### 5. (Caso necess√°rio) Organizar arquivos auxiliares

√â comum bases de dados serem disponibilizadas com arquivos auxiliares. Esses podem incluir notas t√©cnicas, descri√ß√µes de coleta e amostragem, etc. Para ajudar usu√°rios da Base dos Dados terem mais contexto e entenderem melhor os dados, organize todos esses arquivos auxiliares em `/extra/auxiliary_files`.

Fique √† vontade para estruturar sub-pastas como quiser l√° dentro. O que importa √© que fique claro o que s√£o esses arquivos.

### 6. (Caso necess√°rio) Criar tabela dicion√°rio

Muitas vezes, especialmente com bases antigas, h√° m√∫ltiplos dicion√°rios em formatos Excel ou outros. Na Base dos Dados n√≥s unificamos tudo em um √∫nico arquivo em formato `.csv` - um √∫nico dicion√°rio para todas as colunas de todas as tabelas do seu conjunto.

<Tip caption="Detalhes importantes de como construir seu dicion√°rio est√£o no nosso [manual de estilo](style_data)."/>

#### Exemplo: RAIS - Dicion√°rio

O dicion√°rio completo [pode ser consultado
aqui](https://docs.google.com/spreadsheets/d/12Wwp48ZJVux26rCotx43lzdWmVL54JinsNnLIV3jnyM/edit?usp=sharing).
Ele j√° possui a estrutura padr√£o que utilizamos para dicion√°rios.

### 7. Subir tudo no Google Cloud

Tudo pronto! Agora s√≥ falta subir para o Google Cloud e enviar para
revis√£o. Para isso, vamos usar o cliente `basedosdados` (dispon√≠vel em Python) que facilita as configura√ß√µes e etapas do processo.


<Tip caption="Como existe um custo para o armazenamento no storage, para finalizar essa etapa vamos precisar te disponibilizar uma api_key especifica para volunt√°rios para subir os dados em nosso ambiente de desenvolvimento. Assim, entre em nosso [canal no Discord](https://discord.gg/huKWpsVYx4), nos chame em 'quero-contribuir' e marque a `@equipe_dados`"/>

#### Configure suas credenciais localmente
  **7.1** No seu terminal instale nosso cliente: `pip install basedosdados`.

  **7.2** Rode `import basedosdados as bd` no python e siga o passo a passo para configurar localmente com as credenciais de seu projeto no Google Cloud. Preencha as informa√ß√µes conforme a seguir:
```
    * STEP 1: y
    * STEP 2: basedosdados-dev  (colocar o .json passado pela equipe da bd na pasta credentials)
    * STEP 3: y
    * STEP 4: basedosdados-dev
    * STEP 5: https://api.basedosdados.org/api/v1/graphql
```
#### Suba os arquivos na Cloud
Os dados v√£o passar por 3 lugares no Google Cloud:

  * **Storage**: tamb√©m chamado de GCS √© o local onde ser√£o armazenados o arquivos "frios" (arquiteturas, dados, arquivos auxiliares).
  * **BigQuery-DEV-Staging**: tabela que conecta os dados do storage ao projeto basedosdados-dev no bigquery
  * **BigQuery-DEV-Produ√ß√£o**: tabela utilizada paras teste e tratamento via SQL do conjunto de dados

**7.3** Crie a tabela no *bucket do GCS* e *BigQuey-DEV-staging*, usando a API do Python, da seguinte forma:

```python
import basedosdados as bd

DATASET_ID = "dataset_id"  # Nome do dataset
TABLE_ID = "table_id"  # Nome da tabela

tb = bd.Table(dataset_id=DATASET_ID, table_id=TABLE_ID)
``` 


```python
tb.create(
path=path_to_data,  # Caminho para o arquivo csv ou parquet
if_storage_data_exists="raise",
if_table_exists="replace",
source_format="csv",
)
```


<Tip caption="Caso seus dados sejam particionados, o caminho deve apontar para a pasta onde est√£o as parti√ß√µes. No contr√°rio, deve apontar para um arquivo `.csv` (por exemplo, microdados.csv).">

Se o projeto n√£o existir no BigQuery, ele ser√° automaticamente criado

Consulte tamb√©m nossa [API](https://basedosdados.org/docs/api_reference_python) para mais detalhes de cada m√©todo.

</Tip>

**7.4** Crie os arquivos .sql e schema.yml a partir da tabela de arquitetura para rodar a materializa√ß√£o e os testes no dbt (data build-tool):

```python
from databasers_utils import TableArchitecture

arch = TableArchitecture(
    dataset_id="<dataset-id>",
    tables={
        "<table-id>": "URL da arquiterura do Google Sheet",  # Exemplo https://docs.google.com/spreadsheets/d/1K1svie4Gyqe6NnRjBgJbapU5sTsLqXWTQUmTRVIRwQc/edit?usp=drive_link
    },
)

# Cria o yaml file
arch.create_yaml_file()

# Cria os arquivos sql
arch.create_sql_files()

# Atualiza o dbt_project.yml
arch.update_dbt_project()

```

<Tip caption="Caso voc√™ precise, nesse momento voc√™ pode alterar a consulta em SQL para realizar tratamentos finais a partir da tabela `staging`, pode incluir coluna, remover coluna, fazer opera√ß√µes alg√©bricas, substituir strings, etc. O SQL √© o limite!"/>



**7.5** Usando o DBT

Os arquivos sql do dbt usam a macro `set_datalake_project` que indica de qual projeto (basedosdados-staging ou basedosdados-dev) ser√£o consumidos os dados. Ao criar os arquivos usando a fun√ß√£o `create_sql_files` a macro ser√° inserida.

```sql
select
    col_name
from {{ set_datalake_project("<DATASET_ID>_staging.<TABLE_ID>") }}
```


### Materializando o modelo no BigQuery

Materializa um √∫nico modelo pelo nome em basedosdados-dev consumindo os dados de `basedosdados-dev.{table_id}_staging`

```sh
dbt run --select dataset_id__table_id
```

Materializa todos os modelos em uma pasta em basedosdados-dev consumindo os dados de `basedosdados-dev.{table_id}_staging`

```sh
dbt run --select model.dateset_id.dateset_id__table_id
```

Materializa todos os modelos no caminho em basedosdados-dev consumindo os dados de `basedosdados-dev.{table_id}_staging`

```sh
dbt run --select models/dataset_id
```

Materializa um √∫nico modelo pelo caminho do arquivo sql em basedosdados-dev consumindo os dados de `basedosdados-dev.{table_id}_staging`

```sh
dbt run --select models/dataset/table_id.sql
```

### Testando o modelo no BigQuery

Testa um √∫nico modelo

```sh
dbt test --select dataset_id__table_id
```

Testa todos os modelos em uma pasta

```sh
dbt test --select model.dateset_id.dateset_id__table_id
```

Testa todos os modelos no caminho

```sh
dbt test --select models/dataset_id
```

**7.6** Suba os metadados da tabela no site:

<Tip caption="Por enquanto apenas a equipe dados tem permiss√µes de subir os metadados da tabela no site, por isso ser√° necess√°rio entrar em contato conosco. J√° estamos trabalhando para que, num futuro pr√≥ximo, os volunt√°rios tamb√©m possam atualizar dados no site."/>


### 8. Enviar tudo para revis√£o

Ufa, √© isso! Agora s√≥ resta enviar tudo para revis√£o no
[reposit√≥rio](https://github.com/basedosdados/pipelines) da Base dos Dados.

1. Clone o nosso [reposit√≥rio](https://github.com/basedosdados/pipelines) localmente.
2. D√™ um `cd` para a pasta local do reposit√≥rio e abra uma nova branch com `git checkout -b [dataset_id]`. Todas as adi√ß√µes e modifica√ß√µes ser√£o inclu√≠das nessa _branch_.
3. Para cada tabela nova incluir o arquivo com nome `dataset__table_id.sql` na pasta `pipelines/models/dataset_id/` copiando as queries e o schema que voc√™ criou no passo 7.
4. Inclua o seu c√≥digo de captura e limpeza em na pasta `pipelines/models/dataset_id/code`
5. Agora √© s√≥ publicar a branch, abrir o PR com as labels 'table-approve' e marcar a equipe dados para corre√ß√£o

**E agora?** Nossa equipe ir√° revisar os dados e metadados submetidos via Github. Podemos entrar em contato para tirar d√∫vidas ou solicitar mudan√ßas no c√≥digo. Quando tudo estiver OK, fazemos um _merge_ do seu *pull request* e os dados s√£o automaticamente publicados na nossa plataforma!
